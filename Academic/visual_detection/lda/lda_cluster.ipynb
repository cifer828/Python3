{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import re\n",
    "import datetime\n",
    "# % matplotlib inline\n",
    "# % matplotlib auto\n",
    "path_dir = 'C:\\\\Users\\\\zhqch\\\\Documents\\\\code\\\\Python3Projects\\\\visual_detection\\\\lda\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2_2_2_10', '2_2_2_9', '2_2_2_8', '2_2_2_7', '2_2_1_7', '2_2_1_8',\n       '2_2_1_10', '2_2_1_9', '2_2_1_6', '2_2_1_5', '2_2_1_4', '2_2_1_3',\n       '2_2_1_2', '2_2_2_5', '2_2_2_6', '2_2_2_4', '2_3_1_10', '2_3_1_9',\n       '2_3_1_8', '2_3_1_7', '2_3_1_6', '2_3_2_7', '1_2_1_10', '1_3_1_10',\n       '1_3_1_9', '1_3_1_8', '1_3_1_7', '1_3_1_6', '1_3_1_5', '1_3_1_4',\n       '1_3_1_3', '1_3_1_2', '1_3_1_1', '1_3_2_3', '1_2_2_9', '1_2_2_8',\n       '1_2_2_10', '2_2_2_3', '2_2_2_2', '2_2_2_1', '1_2_2_7', '1_2_2_6',\n       '1_2_2_5', '1_2_2_4', '1_2_2_3', '1_2_2_2', '1_2_1_9', '1_1_2_6',\n       '1_1_1_6', '1_3_2_8', '1_3_2_9', '1_2_1_8', '1_2_1_7', '1_2_1_6',\n       '1_2_1_5', '1_2_1_4', '1_3_2_4', '1_3_2_5', '1_3_2_6', '1_3_2_7',\n       '1_3_2_10', '1_1_2_8', '1_1_2_9', '1_1_2_10', '2_2_1_1', '1_2_1_3',\n       '1_2_2_1', '2_3_2_8', '2_3_1_5', '2_3_1_4', '1_2_1_2', '2_1_1_10',\n       '2_1_1_9', '2_1_1_8', '2_1_1_7', '1_2_1_1', '1_1_2_7', '1_1_1_3',\n       '1_1_1_4', '1_1_2_4', '2_3_2_10', '2_3_2_9', '2_1_1_6', '2_3_1_3',\n       '1_1_1_7', '1_1_1_8', '1_1_1_9', '2_3_2_6', '2_3_2_4', '2_3_2_3',\n       '2_3_2_2', '2_3_2_5', '2_1_2_8', '2_3_1_2', '2_1_1_5', '2_1_1_4',\n       '2_1_1_3', '2_1_1_2', '1_1_1_10', '2_1_2_7', '2_1_2_9', '2_1_2_5',\n       '2_3_1_1', '2_1_2_10', '1_1_1_5', '2_1_2_4', '2_1_2_6', '1_1_2_1',\n       '1_1_1_1', '1_1_1_2', '1_1_2_2', '1_1_2_3', '2_1_1_1', '2_3_2_1',\n       '2_1_2_2', '2_1_2_3', '1_1_2_5', '2_1_2_1', '1_3_2_2', '1_3_2_1'],\n      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conflict_df = pd.read_csv('C:\\\\Users\\\\zhqch\\\\Documents\\\\code\\\\Python3Projects\\\\visual_detection\\\\lda\\\\all_conflicts.csv')\n",
    "# os.getcwd()\n",
    "# conflict_df.head()\n",
    "conflict_df = conflict_df.loc[conflict_df['PET'] > 0,:].loc[conflict_df['PET'] < 10,:]\n",
    "conflict_df['PET_level'] = pd.cut(conflict_df['PET'],[0,1,2,3,4,5,6,7,8,9,10], labels=[1,2,3,4,5,6,7,8,9,10])\n",
    "# plt.plot(conflict_df['PET'])\n",
    "conflict_df['Word'] = conflict_df.apply(lambda x: str(x['Conflict_user'])+'_'+str(x['Vel_level']) +'_'+ str(x['Angle_level']) + '_'+str(x['PET_level']), axis =1)\n",
    "conflict_df['Conflict_code'] = conflict_df.apply(lambda x: str(x['Conflict_user']) + '_' + str(x['Ped_id']) + '_' + str(x['Conflict_id']), axis=1)\n",
    "doc_list = conflict_df['Conflict_code'].unique().tolist()\n",
    "documents = []\n",
    "for doc in doc_list:\n",
    "    documents.append(conflict_df.loc[conflict_df['Conflict_code']==doc,'Word'].tolist())\n",
    "# corpus\n",
    "conflict_df['Word'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conflict_df = conflict_df.loc[conflict_df['PET'] > 0,:].loc[conflict_df['PET'] < 10,:]\n",
    "conflict_df['Word'].unique().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建语料的词语词典，每个单独的词语都会被赋予一个索引\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# 使用上面的词典，将转换文档列表（语料）变成 DT 矩阵\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，1个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 1\nthe perplexity of this ldamodel is : 2.403523882692114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，2个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 2\nthe perplexity of this ldamodel is : 2.1320398086665233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，3个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 3\nthe perplexity of this ldamodel is : 1.9454412215872117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，4个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 4\nthe perplexity of this ldamodel is : 1.936705024006207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，5个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 5\nthe perplexity of this ldamodel is : 1.8147547216465953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，6个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 6\nthe perplexity of this ldamodel is : 1.8353425906620338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，7个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 7\nthe perplexity of this ldamodel is : 1.7933843536843759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，8个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 8\nthe perplexity of this ldamodel is : 1.7866870598664648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，9个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 9\nthe perplexity of this ldamodel is : 1.8166527630909461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，10个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 10\nthe perplexity of this ldamodel is : 1.7594454755467082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，11个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 11\nthe perplexity of this ldamodel is : 1.6895616970585767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，12个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 12\nthe perplexity of this ldamodel is : 1.716930229279009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，13个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 13\nthe perplexity of this ldamodel is : 1.7225047583990793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，14个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 14\nthe perplexity of this ldamodel is : 1.7081074470630675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，15个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 15\nthe perplexity of this ldamodel is : 1.6643549034004623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，16个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 16\nthe perplexity of this ldamodel is : 1.6858790778103072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，17个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 17\nthe perplexity of this ldamodel is : 1.6715044478131174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，18个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 18\nthe perplexity of this ldamodel is : 1.6617102832841337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，19个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 19\nthe perplexity of this ldamodel is : 1.6803282171447904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，20个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 20\nthe perplexity of this ldamodel is : 1.6663187690475352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，21个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 21\nthe perplexity of this ldamodel is : 1.73545236169362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，22个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 22\nthe perplexity of this ldamodel is : 1.6726908402308316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，23个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 23\nthe perplexity of this ldamodel is : 1.6657049302471474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，24个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 24\nthe perplexity of this ldamodel is : 1.6658934507470793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，25个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 25\nthe perplexity of this ldamodel is : 1.6511768549176589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，26个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 26\nthe perplexity of this ldamodel is : 1.6574141502736954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，27个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 27\nthe perplexity of this ldamodel is : 1.6702085299169709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，28个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 28\nthe perplexity of this ldamodel is : 1.658576387993926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共训练4782篇文档，29个主题\nthe info of this ldamodel: \n\nnum of testset: 20; size_dictionary: 120; num of topics: 29\nthe perplexity of this ldamodel is : 1.659640953550668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def perplexity(ldamodel, testset, dictionary, size_dictionary, num_topics):\n",
    "    \"\"\"calculate the perplexity of a lda-model\"\"\"\n",
    "    # dictionary : {7822:'deferment', 1841:'circuitry',19202:'fabianism'...]\n",
    "    print ('the info of this ldamodel: \\n')\n",
    "    print ('num of testset: %s; size_dictionary: %s; num of topics: %s'%(len(testset), size_dictionary, num_topics))\n",
    "    prep = 0.0\n",
    "    prob_doc_sum = 0.0\n",
    "    topic_word_list = [] # store the probablity of topic-word:[(u'business', 0.010020942661849608),(u'family', 0.0088027946271537413)...]\n",
    "    for topic_id in range(num_topics):\n",
    "        topic_word = ldamodel.show_topic(topic_id, size_dictionary)\n",
    "        dic = {}\n",
    "        for word, probability in topic_word:\n",
    "            dic[word] = probability\n",
    "        topic_word_list.append(dic)\n",
    "    doc_topics_ist = [] #store the doc-topic tuples:[(0, 0.0006211180124223594),(1, 0.0006211180124223594),...]\n",
    "    for doc in testset:\n",
    "        doc_topics_ist.append(ldamodel.get_document_topics(doc, minimum_probability=0))\n",
    "    testset_word_num = 0\n",
    "    for i in range(len(testset)):\n",
    "        prob_doc = 0.0 # the probablity of the doc\n",
    "        doc = testset[i]\n",
    "        doc_word_num = 0 # the num of words in the doc\n",
    "        for word_id, num in doc:\n",
    "            prob_word = 0.0 # the probablity of the word \n",
    "            doc_word_num += num\n",
    "            word = dictionary[word_id]\n",
    "            for topic_id in range(num_topics):\n",
    "                # cal p(w) : p(w) = sumz(p(z)*p(w|z))\n",
    "                prob_topic = doc_topics_ist[i][topic_id][1]\n",
    "                prob_topic_word = topic_word_list[topic_id][word]\n",
    "                prob_word += prob_topic*prob_topic_word\n",
    "            prob_doc += math.log(prob_word) # p(d) = sum(log(p(w)))\n",
    "        prob_doc_sum += prob_doc\n",
    "        testset_word_num += doc_word_num\n",
    "    prep = math.exp(-prob_doc_sum/testset_word_num) # perplexity = exp(-sum(p(d)/sum(Nd))\n",
    "    print (\"the perplexity of this ldamodel is : %s\"%prep)\n",
    "    return prep\n",
    "\n",
    "# ppl = []\n",
    "# for i in range(1, 30):\n",
    "#     topic_num = i\n",
    "#     doc_num = len(doc_list)\n",
    "#     lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=topic_num, iterations=10000,  alpha=0.01, eta=0.01, minimum_probability=0.001,\n",
    "#                             update_every = 1, chunksize = 100, passes = 1)\n",
    "#     print('共训练%d篇文档，%d个主题' % (doc_num, topic_num))\n",
    "#     ppl.append(perplexity(lda, corpus[:20], dictionary, len(dictionary), topic_num))\n",
    "# pd.DataFrame(ppl).to_csv(path_dir+'perplexity_result.csv')\n",
    "# plt.plot(range(1,5), ppl)\n",
    "# plt.savefig('C:\\\\Users\\\\zhqch\\\\Documents\\\\code\\\\Python3Projects\\\\visual_detection\\\\lda\\\\perplexity.jpg')\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "topic_num=12\n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=topic_num, iterations=10000,  alpha=0.01, eta=0.01, minimum_probability=0.001,\n",
    "                            update_every = 1, chunksize = 100, passes = 1)\n",
    "# 主题分布\n",
    "# print('***************************所有文档主题分布***************************')\n",
    "doc_topics = lda.get_document_topics(corpus)\n",
    "doc2topic_file = 'visual_detection//lda//doc2topic.csv'\n",
    "with open(doc2topic_file, 'w') as f:\n",
    "    f.write('***************************Topic distribution for each document***************************\\n')\n",
    "\n",
    "topic_list = []\n",
    "# 输出每个文档的主题分布\n",
    "for doc_num, topic in enumerate(doc_topics):\n",
    "    one_doc_topic = [0 for _ in range(topic_num)]\n",
    "    for topic_no, topic_probability in topic:\n",
    "        one_doc_topic[topic_no] = topic_probability\n",
    "    topic_list.append(one_doc_topic)\n",
    "topic_df = pd.DataFrame(topic_list, index=doc_list)\n",
    "topic_df.columns = ['Topic %d' % num for num in topic_df.columns]\n",
    "\n",
    "cluster_model = KMeans(n_clusters = 4, n_jobs = 1, max_iter = 1000)\n",
    "cluster_model.fit(topic_list)\n",
    "topic_df['cluster'] = cluster_model.labels_\n",
    "topic_df.to_csv(doc2topic_file, index_label='Conflict')\n",
    "# with open(doc2topic_file, 'a') as f:\n",
    "#             f.write('%s\\t%d\\t%.4f\\n' % (doc_list[doc_num], topic_num, topic_probability))\n",
    "# for topic_idx in topic_nums:\n",
    "#     docs_in_topic = []  # 拥有该主题的所有doc\n",
    "#     for doc_idx in range(doc_num):\n",
    "#         for topic in doc_topics[doc_idx]:\n",
    "#             if topic[0] == topic_idx:\n",
    "#                 docs_in_topic.append([doc_idx, topic[1]])   # [文档id， 主题在该文档的占比]\n",
    "#     docs_sorted = sorted(docs_in_topic, key = operator.itemgetter(1), reverse = True)\n",
    "#     with open('visual_detection/lda/%s.txt' % time_now , 'a') as f:\n",
    "#         temp_writing = '第%d个主题的排名前20个文档：\\n' % topic_idx\n",
    "#         for d in docs_sorted[:21]:\n",
    "#             topic_in_d = doc_topics[d[0]]\n",
    "#             topic_sorted = sorted(topic_in_d, key = operator.itemgetter(1), reverse = True)\n",
    "#             temp_writing += '第%d个文档的主题分布：' % (d[0], ) + '\\n'\n",
    "#             for item in topic_sorted:\n",
    "#                 temp_writing += '{:>8d}'.format(item[0])\n",
    "#             temp_writing += '\\n'\n",
    "#             for item in topic_sorted[:10]:\n",
    "#                temp_writing += '{:>8.4f}'.format(item[1])\n",
    "#             temp_writing += '\\n'\n",
    "#         f.write(temp_writing)\n",
    "    # print('第%d个文档的前%d个主题：' % (doc_id, 10), [item[0] for item in topic_sorted[:10]])\n",
    "    # print( [item[1] for item in topic_sorted[:10]])\n",
    "\n",
    "# 单词分布\n",
    "topic2word = []\n",
    "for topic_id in range(topic_num):\n",
    "    word_prob = []\n",
    "    term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "    term_distribute = term_distribute_all\n",
    "    term_distribute = np.array(term_distribute)\n",
    "    term_id = term_distribute[:, 0].astype(np.int)\n",
    "    for ti,td in zip(term_id, term_distribute):\n",
    "        word_prob.append([dictionary.id2token[ti], round(td[1],3)])\n",
    "    topic2word += list(zip(*word_prob))\n",
    "topic_df = pd.DataFrame(topic2word)\n",
    "topic_df.to_csv('visual_detection/lda/topic2word.csv', index=False)\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "# print('***************************所有主题的单词分布***************************')\n",
    "# with open('visual_detection/lda/topic2word.txt', 'w') as f:\n",
    "#     f.write('***************************所有主题的单词分布***************************\\n')\n",
    "\n",
    "# for topic_id in range(topic_num):\n",
    "#     term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "#     term_distribute = term_distribute_all\n",
    "#     term_distribute = np.array(term_distribute)\n",
    "#     term_id = term_distribute[:, 0].astype(np.int)\n",
    "#     with open('visual_detection/lda/topic2word.txt', 'a') as f:\n",
    "#         temp_writing = ''\n",
    "#         temp_writing += '\\n主题#%d：\\t' % topic_id + '\\n'\n",
    "#         temp_writing += '词：\\t'\n",
    "#         for t in term_id:\n",
    "#             temp_writing += dictionary.id2token[t] + '\\t'\n",
    "#         temp_writing += '\\n概率：\\t'\n",
    "#         for td in term_distribute:\n",
    "#             temp_writing += '{:>8.4f}'.format(td[1])\n",
    "#         f.write(temp_writing)\n",
    "    # print('主题#%d：\\t' % topic_id)\n",
    "    # print('词：\\t',end = ' ')\n",
    "    # for t in term_id:\n",
    "        # print(dictionary.id2token[t],end = ' ')\n",
    "    # print('\\n概率：\\t', term_distribute[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [[1,2],[1,2]]\n",
    "list(zip(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Conflict   Topic 0  Topic 1  Topic 2  Topic 3  Topic 4  Topic 5  \\\n0  2_9732_6891  0.000000      0.0      0.0      0.0      0.0      0.0   \n1  2_9732_6932  0.000000      0.0      0.0      0.0      0.0      0.0   \n2  2_9757_6861  0.718030      0.0      0.0      0.0      0.0      0.0   \n3  2_9757_6891  0.545257      0.0      0.0      0.0      0.0      0.0   \n4  2_9757_6932  0.149792      0.0      0.0      0.0      0.0      0.0   \n\n   Topic 6   Topic 7  Topic 8  Topic 9  Topic 10  Topic 11  cluster  \n0      0.0  0.000000      0.0      0.0  0.992725       0.0        2  \n1      0.0  0.239373      0.0      0.0  0.756646       0.0        2  \n2      0.0  0.000000      0.0      0.0  0.276452       0.0        1  \n3      0.0  0.118932      0.0      0.0  0.332080       0.0        1  \n4      0.0  0.457081      0.0      0.0  0.389235       0.0        1  \n   Ped_id  Time       Ped_x     Ped_y   Ped_vel  Vel_level  Conflict_user  \\\n0    9732  3189  194.668803  35.95238  1.245479          2              2   \n1    9732  3192  194.668803  35.95238  1.248657          2              2   \n2    9732  3195  194.668803  35.95238  1.259601          2              2   \n3    9732  3198  194.668803  35.95238  1.277313          2              2   \n4    9732  3201  194.668803  35.95238  1.301199          2              2   \n\n   Conflict_id                 Angle  Angle_level   Distance       PET  \\\n0         6891  0.328634894887424+0i            2  12.312493  9.885751   \n1         6891  0.197289516172375+0i            2  11.869226  9.505594   \n2         6891  0.025217070991069+0i            2  11.556034  9.174363   \n3         6891  0.170612733168369+0i            2  11.306405  8.851713   \n4         6891  0.357204909138312+0i            2  11.111985  8.539806   \n\n   PET_level      Word Conflict_code  \n0         10  2_2_2_10   2_9732_6891  \n1         10  2_2_2_10   2_9732_6891  \n2         10  2_2_2_10   2_9732_6891  \n3          9   2_2_2_9   2_9732_6891  \n4          9   2_2_2_9   2_9732_6891  \n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('visual_detection//lda//doc2topic.csv')\n",
    "df2 = pd.read_csv('visual_detection//lda//filter_conflicts.csv')\n",
    "print(df1.head())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Ped_id          Time       Ped_x      Ped_y   Ped_vel  \\\nConflict_code                                                          \n1_1018_1061    1018.0  15732.000000  177.842271  45.000000  1.204364   \n1_1024_1061    1024.0  15717.000000  199.655169  48.147100  1.044825   \n1_1031_1061    1031.0  15717.000000  193.749199  40.699813  1.144964   \n1_1122_1177    1122.0  15805.026316  180.542952  53.852900  0.861529   \n1_1138_1177    1138.0  15820.500000  150.066170  46.541353  1.035963   \n\n               Vel_level  Conflict_user  Conflict_id  Angle_level  Distance  \\\nConflict_code                                                                 \n1_1018_1061          2.0            1.0       1061.0     1.244898  6.062096   \n1_1024_1061          2.0            1.0       1061.0     2.000000  7.173504   \n1_1031_1061          2.0            1.0       1061.0     1.153846  7.720688   \n1_1122_1177          2.0            1.0       1177.0     1.552632  6.449632   \n1_1138_1177          2.0            1.0       1177.0     2.000000  5.147103   \n\n                    PET  PET_level  \nConflict_code                       \n1_1018_1061    5.031533   5.551020  \n1_1024_1061    6.865450   7.358974  \n1_1031_1061    6.936710   7.358974  \n1_1122_1177    7.349810   7.894737  \n1_1138_1177    4.958637   5.426471  \n"
     ]
    }
   ],
   "source": [
    "print(df2.groupby('Conflict_code').mean().head())\n",
    "# df = pd.merge(df1,df2,how='inner',left_on='Conflict',right_on='Conflict_id')\n",
    "# print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n  \"\"\"Entry point for launching an IPython kernel.\nC:\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n  \nC:\\Software\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflict     2_9939_4848\nTopic 0         0.497962\nTopic 1       0.00892857\nTopic 2         0.507842\nTopic 3         0.386483\nTopic 4         0.507379\nTopic 5         0.167425\nTopic 6         0.237216\nTopic 7         0.473805\nTopic 8       0.00892857\nTopic 9       0.00892857\nTopic 10         0.99861\nTopic 11        0.249584\ncluster                2\npet_max          9.99896\npet_min          9.99591\npet_mean         9.99649\npet_std          2.89697\ndist_max         32.9467\ndist_std         8.06442\ndist_mean        27.3297\ndist_min         25.8473\nvel_max          4.18307\nvel_mean         2.84111\nvel_min          2.79637\nvel_std         0.818316\ndtype: object\n"
     ]
    }
   ],
   "source": [
    "pet = df2.groupby('Conflict_code')['PET'].agg({'pet_mean':np.mean, 'pet_min':np.min, 'pet_max':np.max, 'pet_std': np.std})\n",
    "dist = df2.groupby('Conflict_code')['Distance'].agg({'dist_mean':np.mean, 'dist_min':np.min, 'dist_max':np.max,'dist_std': np.std})\n",
    "vel =  df2.groupby('Conflict_code')['Ped_vel'].agg({'vel_mean':np.mean, 'vel_min':np.min, 'vel_max':np.max,'vel_std': np.std})\n",
    "df3 = (df1.merge(pet, left_on='Conflict', right_on='Conflict_code')\n",
    " .merge(dist, left_on='Conflict', right_on='Conflict_code')\n",
    " .merge(vel, left_on='Conflict', right_on='Conflict_code'))\n",
    "df3.to_csv('visual_detection//lda//cluster_result.csv', index=False)\n",
    "print(df3.loc[df3['cluster']==2,:].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflict_code\n1_1018_1061    [1_2_1_8, 1_2_1_8, 1_2_1_8, 1_2_1_8, 1_2_1_8, ...\n1_1024_1061    [1_2_2_8, 1_2_2_8, 1_2_2_8, 1_2_2_7, 1_2_2_7, ...\n1_1031_1061    [1_2_1_6, 1_2_1_6, 1_2_1_6, 1_2_1_6, 1_2_1_7, ...\n1_1122_1177    [1_2_2_10, 1_2_2_10, 1_2_1_10, 1_2_1_10, 1_2_1...\n1_1138_1177    [1_2_2_9, 1_2_2_9, 1_2_2_9, 1_2_2_9, 1_2_2_9, ...\n1_1246_2416    [1_2_2_8, 1_2_2_8, 1_2_2_7, 1_2_2_7, 1_2_2_7, ...\n1_1246_2431    [1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2...\n1_1257_2416    [1_2_1_8, 1_2_1_7, 1_2_1_7, 1_2_1_7, 1_2_1_7, ...\n1_1257_2431    [1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2...\n1_1265_2416    [1_2_2_5, 1_2_2_5, 1_2_2_5, 1_2_2_5, 1_2_2_5, ...\n1_1265_2431    [1_2_1_10, 1_2_1_10, 1_2_1_10, 1_2_1_9, 1_2_1_...\n1_1273_2408    [1_3_1_10, 1_3_1_10, 1_3_1_10, 1_3_1_10, 1_3_1...\n1_1273_2416    [1_2_2_8, 1_2_2_8, 1_2_2_7, 1_2_2_6, 1_2_2_5, ...\n1_1273_2431    [1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2...\n1_1311_2416    [1_2_2_5, 1_2_2_5, 1_2_2_5, 1_2_2_5, 1_2_2_5, ...\n1_1311_2431    [1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2_9, 1_2_2_...\n1_1319_2416    [1_2_2_5, 1_2_2_5, 1_2_2_5, 1_2_2_4, 1_2_2_4, ...\n1_1319_2431    [1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2...\n1_1319_2439                       [1_2_2_10, 1_2_2_10, 1_2_2_10]\n1_1330_2416    [1_2_1_9, 1_2_1_9, 1_2_1_8, 1_2_1_8, 1_2_1_7, ...\n1_1330_2439    [1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2...\n1_1338_2408    [1_2_1_10, 1_2_1_10, 1_2_1_10, 1_2_1_10, 1_2_1...\n1_1338_2416    [1_2_1_5, 1_2_1_5, 1_2_1_5, 1_2_1_5, 1_2_1_5, ...\n1_1338_2431    [1_2_2_10, 1_2_2_9, 1_2_2_9, 1_2_2_9, 1_2_2_8,...\n1_1342_2416    [1_2_2_6, 1_2_2_6, 1_2_2_6, 1_2_2_6, 1_2_2_7, ...\n1_1348_2408    [1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2_10, 1_2_2...\n1_1348_2416    [1_2_2_6, 1_2_2_6, 1_2_2_6, 1_2_2_5, 1_2_2_5, ...\n1_1348_2431    [1_2_2_10, 1_2_2_10, 1_2_2_9, 1_2_2_9, 1_2_2_8...\n1_1354_2408    [1_2_2_10, 1_2_2_10, 1_2_2_9, 1_2_2_9, 1_2_2_8...\n1_1354_2416    [1_2_2_5, 1_2_2_5, 1_2_2_5, 1_2_2_5, 1_2_2_5, ...\n                                     ...                        \n2_9444_9556    [2_2_2_10, 2_2_2_10, 2_2_2_9, 2_2_2_9, 2_2_2_9...\n2_9456_9463    [2_2_2_10, 2_2_2_10, 2_2_1_10, 2_2_1_10, 2_2_1...\n2_9456_9471    [2_2_2_10, 2_2_2_10, 2_2_2_10, 2_2_2_9, 2_2_1_...\n2_9456_9485    [2_2_1_10, 2_2_1_10, 2_2_1_10, 2_2_1_9, 2_2_1_...\n2_9456_9491    [2_2_2_9, 2_2_2_9, 2_2_2_9, 2_2_2_9, 2_2_1_9, ...\n2_9456_9498    [2_2_2_8, 2_2_2_8, 2_2_2_7, 2_2_2_7, 2_2_1_7, ...\n2_9732_6891    [2_2_2_10, 2_2_2_10, 2_2_2_10, 2_2_2_9, 2_2_2_...\n2_9732_6932    [2_2_2_10, 2_2_2_9, 2_2_2_9, 2_2_2_8, 2_2_2_8,...\n2_973_6886     [2_2_1_8, 2_2_1_8, 2_2_1_7, 2_2_1_7, 2_2_1_7, ...\n2_973_6895     [2_2_1_7, 2_2_1_7, 2_2_1_7, 2_2_1_7, 2_2_1_7, ...\n2_9757_6861    [2_2_1_10, 2_2_1_10, 2_2_1_9, 2_2_1_9, 2_2_1_8...\n2_9757_6891    [2_2_1_10, 2_2_1_10, 2_2_1_10, 2_2_1_9, 2_2_1_...\n2_9757_6932    [2_2_2_10, 2_2_2_10, 2_2_1_10, 2_2_1_9, 2_2_1_...\n2_9826_6861    [2_2_1_10, 2_2_1_8, 2_2_1_7, 2_2_1_6, 2_2_1_5,...\n2_9826_6891    [2_2_1_10, 2_2_1_9, 2_2_1_9, 2_2_1_8, 2_2_1_7,...\n2_9826_6932    [2_2_2_8, 2_2_2_7, 2_2_2_7, 2_2_2_6, 2_2_2_6, ...\n2_9845_5554    [2_2_2_5, 2_2_2_4, 2_2_2_4, 2_2_2_4, 2_2_2_4, ...\n2_9845_5590    [2_2_1_10, 2_2_1_10, 2_2_1_10, 2_2_1_9, 2_2_1_...\n2_9845_7103    [2_3_1_10, 2_3_1_9, 2_3_1_9, 2_3_1_8, 2_3_1_8,...\n2_9845_7122    [2_2_2_10, 2_2_2_10, 2_2_2_9, 2_2_2_9, 2_2_2_9...\n2_9879_49      [2_2_2_5, 2_2_2_5, 2_2_2_5, 2_2_2_5, 2_2_1_5, ...\n2_9887_4708                       [2_2_1_10, 2_2_1_10, 2_2_1_10]\n2_9887_4861    [2_2_2_10, 2_2_2_10, 2_2_2_9, 2_2_2_9, 2_2_2_8...\n2_9899_49      [2_2_1_7, 2_2_1_7, 2_2_1_7, 2_2_1_6, 2_2_1_6, ...\n2_9910_49      [2_2_2_8, 2_2_2_8, 2_2_2_8, 2_2_2_7, 2_2_2_7, ...\n2_9939_4848    [2_2_2_8, 2_2_2_8, 2_2_2_7, 2_2_2_7, 2_2_2_7, ...\n2_9939_4861    [2_2_1_10, 2_2_1_10, 2_2_1_10, 2_2_1_10, 2_2_1...\n2_9957_4848    [2_2_1_7, 2_2_1_7, 2_2_1_6, 2_2_1_6, 2_2_1_6, ...\n2_9957_58      [2_2_2_10, 2_2_2_10, 2_2_2_10, 2_2_2_9, 2_2_2_...\n2_9980_58      [2_2_1_10, 2_2_1_10, 2_2_2_9, 2_2_2_9, 2_2_2_8...\nName: Word, Length: 4782, dtype: object\n"
     ]
    }
   ],
   "source": [
    "conflict_df = pd.read_csv(path_dir + 'filter_conflicts.csv')\n",
    "g = conflict_df.groupby('Conflict_code')['Word'].apply(list)\n",
    "for idx, row in g.iterrows():\n",
    "    print(idx, row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Conflict_code'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3077\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3078\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Conflict_code'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-53df1049b28a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconflict_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Conflict_code'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconflict_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mset_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   3907\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3908\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3909\u001b[1;33m                 \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3910\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3911\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2687\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2688\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2693\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2695\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2697\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2489\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   4113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4114\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Software\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3078\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Conflict_code'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
